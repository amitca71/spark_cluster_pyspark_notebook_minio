{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bce27ad-2ef9-42c1-82a9-15ced3f15f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-90c6ecba-15c9-4c47-865b-1540d3435b75;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;0.8.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.819 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/0.8.0/delta-core_2.12-0.8.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;0.8.0!delta-core_2.12.jar (1895ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.2/spark-sql-kafka-0-10_2.12-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2!spark-sql-kafka-0-10_2.12.jar (430ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.819/aws-java-sdk-bundle-1.11.819.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.819!aws-java-sdk-bundle.jar (58699ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.2.0!hadoop-aws.jar (308ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4/4.7/antlr4-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4;4.7!antlr4.jar (588ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.7!antlr4-runtime.jar (203ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr-runtime;3.5.2!antlr-runtime.jar (129ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/ST4/4.0.8/ST4-4.0.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#ST4;4.0.8!ST4.jar (208ms)\n",
      "downloading https://repo1.maven.org/maven2/org/abego/treelayout/org.abego.treelayout.core/1.0.3/org.abego.treelayout.core-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.abego.treelayout#org.abego.treelayout.core;1.0.3!org.abego.treelayout.core.jar(bundle) (82ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/javax.json/1.0.4/javax.json-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish#javax.json;1.0.4!javax.json.jar(bundle) (105ms)\n",
      "downloading https://repo1.maven.org/maven2/com/ibm/icu/icu4j/58.2/icu4j-58.2.jar ...\n",
      "\t[SUCCESSFUL ] com.ibm.icu#icu4j;58.2!icu4j.jar (7162ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.0.2/spark-token-provider-kafka-0-10_2.12-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2!spark-token-provider-kafka-0-10_2.12.jar (91ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.4.1/kafka-clients-2.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.4.1!kafka-clients.jar (2231ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (181ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (76ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.4.4-3/zstd-jni-1.4.4-3.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.4.4-3!zstd-jni.jar (5424ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (778ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.2!snappy-java.jar(bundle) (2088ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (138ms)\n",
      ":: resolution report :: resolve 31073ms :: artifacts dl 80833ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.819 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;0.8.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 by [com.amazonaws#aws-java-sdk-bundle;1.11.819] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   20  |   19  |   19  |   1   ||   19  |   19  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-90c6ecba-15c9-4c47-865b-1540d3435b75\n",
      "\tconfs: [default]\n",
      "\t19 artifacts copied, 0 already retrieved (197912kB/219ms)\n",
      "21/10/17 19:13:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setMaster(\"spark://spark:7077\") \n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", 'http://s3:9000') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", 'minio') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", 'minio123') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4e81ba-ae6f-4226-b3e5-cc4d050f7e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting koalas==1.8.1\n",
      "  Downloading koalas-1.8.1-py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 893 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.9/site-packages (from koalas==1.8.1) (1.20.3)\n",
      "Requirement already satisfied: pandas>=0.23.2 in /opt/conda/lib/python3.9/site-packages (from koalas==1.8.1) (1.3.3)\n",
      "Requirement already satisfied: pyarrow>=0.10 in /opt/conda/lib/python3.9/site-packages (from koalas==1.8.1) (5.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23.2->koalas==1.8.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23.2->koalas==1.8.1) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=0.23.2->koalas==1.8.1) (1.16.0)\n",
      "Installing collected packages: koalas\n",
      "Successfully installed koalas-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install koalas==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "486c144e-5f0c-4e84-a702-12a9f29faef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/17 19:14:16 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "21/10/17 19:14:17 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    name|color|\n",
      "+--------+-----+\n",
      "|   Luigi|Green|\n",
      "|Princess| Pink|\n",
      "|   Mario|  Red|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "OBJECTURL_TEST = 's3a://test-container/playground/colors-test' + str(time.time()) + '.csv'\n",
    "rdd = sc.parallelize([('Mario', 'Red'), ('Luigi', 'Green'), ('Princess', 'Pink')])\n",
    "rdd.toDF(['name', 'color']).write.csv(OBJECTURL_TEST, header=True)\n",
    "\n",
    "# Read the data back from MinIO\n",
    "gnames_df = spark.read.format('csv').option('header', True) \\\n",
    "    .load(OBJECTURL_TEST)\n",
    "gnames_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc16fcc-4e9e-43c5-85b7-4c1f90d7c9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "DELTA_URL='s3a://test-container/playground/delts-colors-test' + str(time.time())\n",
    "gnames_df.write.format(\"delta\").save(DELTA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eac4e11-027a-4a9f-820a-16fb74d95be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(0, 5)\n",
    "#data.write.format(\"delta\").save(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1afaad5-9379-499f-9e1d-9bef4dca173b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "s3a://test-container/playground/to_overide already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70/3427780955.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://test-container/playground/to_overide\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: s3a://test-container/playground/to_overide already exists."
     ]
    }
   ],
   "source": [
    "data.write.format(\"delta\").save(\"s3a://test-container/playground/to_overide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "538ec175-bf98-4bfd-b0ae-5aa8777a5944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://test-container/playground/to_overide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a0c351-40e6-416c-8dff-c58623c4ebaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fdb7f69-501d-4c60-bdc0-f4747827bded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "from databricks import koalas as ks\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9ea596c-5197-4227-89ba-f4699106a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ks.Series([1, 3, 5, np.nan, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb43e847-b267-4e25-a3e1-2b0b69b7d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = ks.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7fb304b-41fc-4b5d-be18-fc728e1e67d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>six</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a    b      c\n",
       "10  1  100    one\n",
       "20  2  200    two\n",
       "30  3  300  three\n",
       "40  4  400   four\n",
       "50  5  500   five\n",
       "60  6  600    six"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac7ddd68-8aaf-44ef-a853-e4ef9318e09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'databricks.koalas.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>-0.711055</td>\n",
       "      <td>0.225627</td>\n",
       "      <td>1.028010</td>\n",
       "      <td>0.861587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>-0.698692</td>\n",
       "      <td>-0.668969</td>\n",
       "      <td>0.513985</td>\n",
       "      <td>-0.506543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>0.770722</td>\n",
       "      <td>-1.601294</td>\n",
       "      <td>0.849015</td>\n",
       "      <td>-0.238955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>1.596659</td>\n",
       "      <td>-0.834587</td>\n",
       "      <td>-0.131964</td>\n",
       "      <td>-0.216534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>0.680701</td>\n",
       "      <td>-1.293159</td>\n",
       "      <td>0.585404</td>\n",
       "      <td>1.583542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-06</th>\n",
       "      <td>-0.689997</td>\n",
       "      <td>-1.680504</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.245247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   A         B         C         D\n",
       "2013-01-01 -0.711055  0.225627  1.028010  0.861587\n",
       "2013-01-02 -0.698692 -0.668969  0.513985 -0.506543\n",
       "2013-01-03  0.770722 -1.601294  0.849015 -0.238955\n",
       "2013-01-04  1.596659 -0.834587 -0.131964 -0.216534\n",
       "2013-01-05  0.680701 -1.293159  0.585404  1.583542\n",
       "2013-01-06 -0.689997 -1.680504  0.008660  0.245247"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = pd.date_range('20130101', periods=6)\n",
    "pdf = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n",
    "kdf = ks.from_pandas(pdf)\n",
    "print(type(kdf))\n",
    "kdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cdfc358-de4a-4ed8-93d1-6142d2ecbfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------------------+--------------------+\n",
      "|                  A|                  B|                   C|                   D|\n",
      "+-------------------+-------------------+--------------------+--------------------+\n",
      "|-0.7110552325690844| 0.2256266465605143|   1.028009602206058|  0.8615871414898003|\n",
      "|-0.6986918104874763|-0.6689693578564733|  0.5139848245778735| -0.5065427068760201|\n",
      "| 0.7707221165142124|-1.6012937107688066|  0.8490154748686538|-0.23895543811339473|\n",
      "| 1.5966585873695769|-0.8345871507597107|-0.13196437680788964|-0.21653363857988714|\n",
      "| 0.6807005567050425| -1.293159005368727|  0.5854040626477828|  1.5835416305656298|\n",
      "|-0.6899966854522469|-1.6805036398110058|0.008660405453540977| 0.24524722192265716|\n",
      "+-------------------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.createDataFrame(pdf)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "856bf484-8341-4c01-b89f-390c7eb0829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = sdf.to_koalas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96fb0a05-a9b5-4352-a424-3ad2c0c08be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(A=-0.7110552325690844, B=0.2256266465605143, C=1.028009602206058, D=0.8615871414898003),\n",
       " Row(A=-0.6986918104874763, B=-0.6689693578564733, C=0.5139848245778735, D=-0.5065427068760201),\n",
       " Row(A=0.7707221165142124, B=-1.6012937107688066, C=0.8490154748686538, D=-0.23895543811339473),\n",
       " Row(A=1.5966585873695769, B=-0.8345871507597107, C=-0.13196437680788964, D=-0.21653363857988714),\n",
       " Row(A=0.6807005567050425, B=-1.293159005368727, C=0.5854040626477828, D=1.5835416305656298),\n",
       " Row(A=-0.6899966854522469, B=-1.6805036398110058, C=0.008660405453540977, D=0.24524722192265716)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d34c9e68-c87c-4830-9335-b80ba2987513",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.to_parquet(\"s3a://test-container/playground/kdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "966f4818-f300-49b1-afaf-c3de37fa03c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/17 19:24:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.711055</td>\n",
       "      <td>0.225627</td>\n",
       "      <td>1.028010</td>\n",
       "      <td>0.861587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.698692</td>\n",
       "      <td>-0.668969</td>\n",
       "      <td>0.513985</td>\n",
       "      <td>-0.506543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.770722</td>\n",
       "      <td>-1.601294</td>\n",
       "      <td>0.849015</td>\n",
       "      <td>-0.238955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.596659</td>\n",
       "      <td>-0.834587</td>\n",
       "      <td>-0.131964</td>\n",
       "      <td>-0.216534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.680701</td>\n",
       "      <td>-1.293159</td>\n",
       "      <td>0.585404</td>\n",
       "      <td>1.583542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.689997</td>\n",
       "      <td>-1.680504</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.245247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C         D\n",
       "0 -0.711055  0.225627  1.028010  0.861587\n",
       "1 -0.698692 -0.668969  0.513985 -0.506543\n",
       "2  0.770722 -1.601294  0.849015 -0.238955\n",
       "3  1.596659 -0.834587 -0.131964 -0.216534\n",
       "4  0.680701 -1.293159  0.585404  1.583542\n",
       "5 -0.689997 -1.680504  0.008660  0.245247"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks.read_parquet(\"s3a://test-container/playground/kdf\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f040a36-7aaf-4861-9992-1f0621546382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70/4196962112.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_delta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://test-container/playground/delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'mode'"
     ]
    }
   ],
   "source": [
    "kdf.to_delta(\"s3a://test-container/playground/delta\", mode=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1563acc-f892-44e7-abc0-4f11c9517bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/13 08:11:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.287319</td>\n",
       "      <td>1.309503</td>\n",
       "      <td>-0.601484</td>\n",
       "      <td>1.396977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.613454</td>\n",
       "      <td>0.112073</td>\n",
       "      <td>1.067040</td>\n",
       "      <td>-1.363565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.422918</td>\n",
       "      <td>0.160863</td>\n",
       "      <td>-0.159296</td>\n",
       "      <td>-0.201997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.007683</td>\n",
       "      <td>-0.022153</td>\n",
       "      <td>0.390490</td>\n",
       "      <td>0.615409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.627739</td>\n",
       "      <td>-2.077160</td>\n",
       "      <td>-0.956193</td>\n",
       "      <td>0.758404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.193660</td>\n",
       "      <td>1.829280</td>\n",
       "      <td>0.362110</td>\n",
       "      <td>-0.386733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C         D\n",
       "0  0.287319  1.309503 -0.601484  1.396977\n",
       "1  0.613454  0.112073  1.067040 -1.363565\n",
       "2  0.422918  0.160863 -0.159296 -0.201997\n",
       "3 -1.007683 -0.022153  0.390490  0.615409\n",
       "4 -0.627739 -2.077160 -0.956193  0.758404\n",
       "5 -0.193660  1.829280  0.362110 -0.386733"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks.read_delta(\"s3a://test-container/playground/delta\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405926c-7a24-4046-838e-2b185b12bd99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
