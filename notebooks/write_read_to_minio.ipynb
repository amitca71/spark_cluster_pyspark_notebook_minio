{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bce27ad-2ef9-42c1-82a9-15ced3f15f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.databricks#dbutils-api_2.12 added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3c7fd4d7-a836-49b3-9a7a-257944447bc6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#dbutils-api_2.12;0.0.5 in central\n",
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.819 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/databricks/dbutils-api_2.12/0.0.5/dbutils-api_2.12-0.0.5.jar ...\n",
      "\t[SUCCESSFUL ] com.databricks#dbutils-api_2.12;0.0.5!dbutils-api_2.12.jar (238ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/1.0.0/delta-core_2.12-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;1.0.0!delta-core_2.12.jar (2050ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.2/spark-sql-kafka-0-10_2.12-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2!spark-sql-kafka-0-10_2.12.jar (530ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.819/aws-java-sdk-bundle-1.11.819.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.819!aws-java-sdk-bundle.jar (193206ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.2.0!hadoop-aws.jar (294ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4/4.7/antlr4-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4;4.7!antlr4.jar (1401ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.7!antlr4-runtime.jar (251ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr-runtime;3.5.2!antlr-runtime.jar (252ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/ST4/4.0.8/ST4-4.0.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#ST4;4.0.8!ST4.jar (266ms)\n",
      "downloading https://repo1.maven.org/maven2/org/abego/treelayout/org.abego.treelayout.core/1.0.3/org.abego.treelayout.core-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.abego.treelayout#org.abego.treelayout.core;1.0.3!org.abego.treelayout.core.jar(bundle) (187ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/javax.json/1.0.4/javax.json-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish#javax.json;1.0.4!javax.json.jar(bundle) (197ms)\n",
      "downloading https://repo1.maven.org/maven2/com/ibm/icu/icu4j/58.2/icu4j-58.2.jar ...\n",
      "\t[SUCCESSFUL ] com.ibm.icu#icu4j;58.2!icu4j.jar (6009ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.0.2/spark-token-provider-kafka-0-10_2.12-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2!spark-token-provider-kafka-0-10_2.12.jar (229ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.4.1/kafka-clients-2.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.4.1!kafka-clients.jar (2512ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (269ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (176ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.4.4-3/zstd-jni-1.4.4-3.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.4.4-3!zstd-jni.jar (3127ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (6820ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.2!snappy-java.jar(bundle) (8759ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (301ms)\n",
      ":: resolution report :: resolve 36281ms :: artifacts dl 227096ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.819 from central in [default]\n",
      "\tcom.databricks#dbutils-api_2.12;0.0.5 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 by [com.amazonaws#aws-java-sdk-bundle;1.11.819] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   20  |   20  |   1   ||   20  |   20  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3c7fd4d7-a836-49b3-9a7a-257944447bc6\n",
      "\tconfs: [default]\n",
      "\t20 artifacts copied, 0 already retrieved (198147kB/232ms)\n",
      "21/10/18 11:58:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.setMaster(\"spark://spark:7077\") \n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", 'http://s3:9000') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", 'minio') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", 'minio123') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.fast.upload\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be4e81ba-ae6f-4226-b3e5-cc4d050f7e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting koalas==1.8.1\n",
      "  Downloading koalas-1.8.1-py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 302 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=0.10 in /opt/conda/lib/python3.9/site-packages (from koalas==1.8.1) (5.0.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.9/site-packages (from koalas==1.8.1) (1.20.3)\n",
      "Requirement already satisfied: pandas>=0.23.2 in /opt/conda/lib/python3.9/site-packages (from koalas==1.8.1) (1.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23.2->koalas==1.8.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.23.2->koalas==1.8.1) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=0.23.2->koalas==1.8.1) (1.16.0)\n",
      "Installing collected packages: koalas\n",
      "Successfully installed koalas-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install koalas==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "486c144e-5f0c-4e84-a702-12a9f29faef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/18 11:59:09 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "21/10/18 11:59:10 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    name|color|\n",
      "+--------+-----+\n",
      "|   Luigi|Green|\n",
      "|Princess| Pink|\n",
      "|   Mario|  Red|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "OBJECTURL_TEST = 's3a://test-container/playground/colors-test' + str(time.time()) + '.csv'\n",
    "rdd = sc.parallelize([('Mario', 'Red'), ('Luigi', 'Green'), ('Princess', 'Pink')])\n",
    "rdd.toDF(['name', 'color']).write.csv(OBJECTURL_TEST, header=True)\n",
    "\n",
    "# Read the data back from MinIO\n",
    "gnames_df = spark.read.format('csv').option('header', True) \\\n",
    "    .load(OBJECTURL_TEST)\n",
    "gnames_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc16fcc-4e9e-43c5-85b7-4c1f90d7c9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "DELTA_URL='s3a://test-container/playground/delts-colors-test' + str(time.time())\n",
    "gnames_df.write.format(\"delta\").save(DELTA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eac4e11-027a-4a9f-820a-16fb74d95be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.range(0, 5)\n",
    "#data.write.format(\"delta\").save(\"/tmp/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1afaad5-9379-499f-9e1d-9bef4dca173b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "s3a://test-container/playground/to_overide already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_85/3427780955.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://test-container/playground/to_overide\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: s3a://test-container/playground/to_overide already exists."
     ]
    }
   ],
   "source": [
    "data.write.format(\"delta\").save(\"s3a://test-container/playground/to_overide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538ec175-bf98-4bfd-b0ae-5aa8777a5944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://test-container/playground/to_overide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fdb7f69-501d-4c60-bdc0-f4747827bded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "from databricks import koalas as ks\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9ea596c-5197-4227-89ba-f4699106a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ks.Series([1, 3, 5, np.nan, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb43e847-b267-4e25-a3e1-2b0b69b7d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = ks.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7fb304b-41fc-4b5d-be18-fc728e1e67d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>six</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a    b      c\n",
       "10  1  100    one\n",
       "20  2  200    two\n",
       "30  3  300  three\n",
       "40  4  400   four\n",
       "50  5  500   five\n",
       "60  6  600    six"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac7ddd68-8aaf-44ef-a853-e4ef9318e09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'databricks.koalas.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>-0.567488</td>\n",
       "      <td>1.246976</td>\n",
       "      <td>2.674164</td>\n",
       "      <td>-1.742257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>-0.248967</td>\n",
       "      <td>0.544746</td>\n",
       "      <td>-0.656479</td>\n",
       "      <td>-0.070499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>-2.332652</td>\n",
       "      <td>1.391693</td>\n",
       "      <td>2.880239</td>\n",
       "      <td>0.131104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.291100</td>\n",
       "      <td>0.732094</td>\n",
       "      <td>-1.719156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>0.412345</td>\n",
       "      <td>-0.583493</td>\n",
       "      <td>-0.631307</td>\n",
       "      <td>0.567352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-06</th>\n",
       "      <td>1.304312</td>\n",
       "      <td>1.220635</td>\n",
       "      <td>0.154229</td>\n",
       "      <td>0.479693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   A         B         C         D\n",
       "2013-01-01 -0.567488  1.246976  2.674164 -1.742257\n",
       "2013-01-02 -0.248967  0.544746 -0.656479 -0.070499\n",
       "2013-01-03 -2.332652  1.391693  2.880239  0.131104\n",
       "2013-01-04  0.587629  0.291100  0.732094 -1.719156\n",
       "2013-01-05  0.412345 -0.583493 -0.631307  0.567352\n",
       "2013-01-06  1.304312  1.220635  0.154229  0.479693"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = pd.date_range('20130101', periods=6)\n",
    "pdf = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n",
    "kdf = ks.from_pandas(pdf)\n",
    "print(type(kdf))\n",
    "kdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cdfc358-de4a-4ed8-93d1-6142d2ecbfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+--------------------+\n",
      "|                  A|                  B|                  C|                   D|\n",
      "+-------------------+-------------------+-------------------+--------------------+\n",
      "|-0.5674881744018757| 1.2469762451373825| 2.6741639830585426| -1.7422567336917387|\n",
      "|-0.2489666274140646| 0.5447464950207793|-0.6564786772071499|-0.07049947932153174|\n",
      "|-2.3326518183198126| 1.3916932731147502| 2.8802387758120496| 0.13110390738721153|\n",
      "| 0.5876291654742538|0.29110046741344475| 0.7320941655374227| -1.7191555218947696|\n",
      "| 0.4123449678875732|-0.5834932906851374|-0.6313068997974723|  0.5673516990549402|\n",
      "|  1.304312202950433| 1.2206351427425046|0.15422910056417558|  0.4796931463718705|\n",
      "+-------------------+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.createDataFrame(pdf)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "856bf484-8341-4c01-b89f-390c7eb0829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf = sdf.to_koalas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96fb0a05-a9b5-4352-a424-3ad2c0c08be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(A=-0.5674881744018757, B=1.2469762451373825, C=2.6741639830585426, D=-1.7422567336917387),\n",
       " Row(A=-0.2489666274140646, B=0.5447464950207793, C=-0.6564786772071499, D=-0.07049947932153174),\n",
       " Row(A=-2.3326518183198126, B=1.3916932731147502, C=2.8802387758120496, D=0.13110390738721153),\n",
       " Row(A=0.5876291654742538, B=0.29110046741344475, C=0.7320941655374227, D=-1.7191555218947696),\n",
       " Row(A=0.4123449678875732, B=-0.5834932906851374, C=-0.6313068997974723, D=0.5673516990549402),\n",
       " Row(A=1.304312202950433, B=1.2206351427425046, C=0.15422910056417558, D=0.4796931463718705)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d34c9e68-c87c-4830-9335-b80ba2987513",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdf.to_parquet(\"s3a://test-container/playground/kdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "966f4818-f300-49b1-afaf-c3de37fa03c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/18 12:00:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.567488</td>\n",
       "      <td>1.246976</td>\n",
       "      <td>2.674164</td>\n",
       "      <td>-1.742257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.248967</td>\n",
       "      <td>0.544746</td>\n",
       "      <td>-0.656479</td>\n",
       "      <td>-0.070499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.332652</td>\n",
       "      <td>1.391693</td>\n",
       "      <td>2.880239</td>\n",
       "      <td>0.131104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.291100</td>\n",
       "      <td>0.732094</td>\n",
       "      <td>-1.719156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.412345</td>\n",
       "      <td>-0.583493</td>\n",
       "      <td>-0.631307</td>\n",
       "      <td>0.567352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.304312</td>\n",
       "      <td>1.220635</td>\n",
       "      <td>0.154229</td>\n",
       "      <td>0.479693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C         D\n",
       "0 -0.567488  1.246976  2.674164 -1.742257\n",
       "1 -0.248967  0.544746 -0.656479 -0.070499\n",
       "2 -2.332652  1.391693  2.880239  0.131104\n",
       "3  0.587629  0.291100  0.732094 -1.719156\n",
       "4  0.412345 -0.583493 -0.631307  0.567352\n",
       "5  1.304312  1.220635  0.154229  0.479693"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks.read_parquet(\"s3a://test-container/playground/kdf\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f040a36-7aaf-4861-9992-1f0621546382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "kdf.to_delta(\"s3a://test-container/playground/delta_partitioned\", mode='overwrite', partition_cols=[\"A\",\"B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1563acc-f892-44e7-abc0-4f11c9517bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/18 12:02:03 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.711055</td>\n",
       "      <td>0.225627</td>\n",
       "      <td>1.028010</td>\n",
       "      <td>0.861587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.698692</td>\n",
       "      <td>-0.668969</td>\n",
       "      <td>0.513985</td>\n",
       "      <td>-0.506543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.770722</td>\n",
       "      <td>-1.601294</td>\n",
       "      <td>0.849015</td>\n",
       "      <td>-0.238955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.596659</td>\n",
       "      <td>-0.834587</td>\n",
       "      <td>-0.131964</td>\n",
       "      <td>-0.216534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.680701</td>\n",
       "      <td>-1.293159</td>\n",
       "      <td>0.585404</td>\n",
       "      <td>1.583542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.689997</td>\n",
       "      <td>-1.680504</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.245247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C         D\n",
       "0 -0.711055  0.225627  1.028010  0.861587\n",
       "1 -0.698692 -0.668969  0.513985 -0.506543\n",
       "2  0.770722 -1.601294  0.849015 -0.238955\n",
       "3  1.596659 -0.834587 -0.131964 -0.216534\n",
       "4  0.680701 -1.293159  0.585404  1.583542\n",
       "5 -0.689997 -1.680504  0.008660  0.245247"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks.read_delta(\"s3a://test-container/playground/delta\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ed627b8-51e2-48d3-a3ad-76282b3baff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta.tables import DeltaTable\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f39da425-79a6-4b7b-9ed9-cdb78f3b9484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.range(8)\n",
    "data = data.withColumn(\"value\", data.id + random.randint(0, 5000))\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://test-container/playground/delta-streaming/delta-table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bc78bdc-d705-4a23-8143-99987e910770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### Streaming write ######\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/18 12:03:27 WARN TaskSetManager: Lost task 41.0 in stage 71.0 (TID 880) (192.168.0.4 executor 1): TaskKilled (Stage cancelled)\n",
      "21/10/18 12:03:27 WARN TaskSetManager: Lost task 28.0 in stage 71.0 (TID 881) (192.168.0.2 executor 0): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "# Stream writes to the table\n",
    "print(\"####### Streaming write ######\")\n",
    "streamingDf = spark.readStream.format(\"rate\").load()\n",
    "stream = streamingDf.selectExpr(\"value as id\").writeStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"s3a://test-container/playground/delta-streaming/checkpoint\")\\\n",
    "    .start(\"s3a://test-container/playground/delta-streaming/delta-table2\")\n",
    "stream.awaitTermination(10)\n",
    "stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1b2d73b-8074-4ec2-bdc2-6059c03960f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Reading from stream ######\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/18 12:03:32 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7effab11-6adc-4c4e-970f-8dfaaf5f2a1b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  7|\n",
      "|  9|\n",
      "| 11|\n",
      "| 13|\n",
      "| 15|\n",
      "| 17|\n",
      "| 19|\n",
      "| 21|\n",
      "| 23|\n",
      "| 25|\n",
      "| 27|\n",
      "| 29|\n",
      "| 31|\n",
      "| 33|\n",
      "| 35|\n",
      "| 37|\n",
      "| 39|\n",
      "| 41|\n",
      "| 43|\n",
      "| 45|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stream reads from a table\n",
    "print(\"##### Reading from stream ######\")\n",
    "stream2 = spark.readStream.format(\"delta\").load(\"s3a://test-container/playground/delta-streaming/delta-table2\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .start()\n",
    "stream2.awaitTermination(10)\n",
    "stream2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71689bbc-453e-44f4-9ad9-e4a8e0fb82a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### Streaming upgrades in update mode ########\n"
     ]
    }
   ],
   "source": [
    "print(\"####### Streaming upgrades in update mode ########\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7712c105-2dcf-42a9-ae04-6daa80db7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upsert microBatchOutputDF into Delta Lake table using merge\n",
    "def upsertToDelta(microBatchOutputDF, batchId):\n",
    "    t = deltaTable.alias(\"t\").merge(microBatchOutputDF.alias(\"s\"), \"s.id = t.id\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b796e06e-d46d-47d4-8f66-2f032fb67d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############  Original Delta Table ###############\n",
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  4| 4953|\n",
      "|  5| 4954|\n",
      "|  6| 4955|\n",
      "|  7| 4956|\n",
      "|  0| 4949|\n",
      "|  1| 4950|\n",
      "|  2| 4951|\n",
      "|  3| 4952|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streamingAggregatesDF = spark.readStream.format(\"rate\").load()\\\n",
    "    .withColumn(\"id\", col(\"value\") % 10)\\\n",
    "    .drop(\"timestamp\")\n",
    "\n",
    "# Write the output of a streaming aggregation query into Delta Lake table\n",
    "deltaTable = DeltaTable.forPath(spark, \"s3a://test-container/playground/delta-streaming/delta-table\")\n",
    "print(\"#############  Original Delta Table ###############\")\n",
    "deltaTable.toDF().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8176b3e9-51fb-4bcd-a70b-dd4702a7dbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/18 12:03:53 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-0de633d6-cc00-46e9-8fe3-8773d433cd5e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "[Stage 119:============================================>          (40 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### DeltaTable after streaming upsert #########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "stream3 = streamingAggregatesDF.writeStream\\\n",
    "    .format(\"delta\") \\\n",
    "    .foreachBatch(upsertToDelta) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "stream3.awaitTermination(10)\n",
    "stream3.stop()\n",
    "print(\"########### DeltaTable after streaming upsert #########\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155970e-ca3b-41a9-83f9-8cc7979fc842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115d7f5-c138-4cea-9279-8100f3aabd95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
